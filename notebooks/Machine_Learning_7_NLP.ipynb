{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing : Classic to Deep Methods for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag-Of-Word and TF-IDF:\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2020/02/quick-introduction-bag-of-words-bow-tf-idf/\n",
    "\n",
    "Recurrent Neural Networks (RNNs):\n",
    "\n",
    "https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9\n",
    "\n",
    "Long Short Term Memory networks (LSTMs):\n",
    "\n",
    "https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21\n",
    "\n",
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "Word embeddings:\n",
    "\n",
    "http://jalammar.github.io/illustrated-word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#TOFILL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we are going to tackle the sentiment analysis problem, a *text classification* problem. The idea is pretty simple : we want to automatically predict whether a text expresses positive or negative sentiments. To do so we will use the IMDB dataset, that contains 50000 movie reviews from the www.imdb.com website, and their corresponding sentiment : positive or negative. It is thus a binary classification problem, where we want to predict a binary target $y \\in \\{0,1\\}$. We will go through different ways of encoding a text in a vectorial form $x \\in \\mathbb{R}^d$, as well as different classification models, from classic ways to modern deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset and explore a bit the data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and print the dataset\n",
    "imdb_dataset_original=pd.read_csv('../data/IMDB Dataset.csv')\n",
    "imdb_dataset = imdb_dataset_original.copy()\n",
    "imdb_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print first review:\n",
    "print(imdb_dataset[\"review\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the two classes size\n",
    "imdb_dataset['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "As you can see the text is quite messy, and before encoding our text into features, we are going to go through different preprocessing steps in order to clean it:\n",
    "* Removing the HTML tags.\n",
    "* Removing other special characters : this means all non alphanumeric characters, including punctuation.\n",
    "* Lowercase the text.\n",
    "* Tokenization : split a text as a list of words now called tokens.\n",
    "* Stemming : removing all the suffixes from conjugation, plural, ... In order to bring a word back to its root form. For example.\n",
    "* Removing stopwords : words like 'to', 'a', 'the', ... are called stopwords, we remove them as they are too frequent words and generally just add noise.\n",
    "\n",
    "Fill the following functions to perform each of these steps. You are free to use the libraries of your choice to do so. Try to not reinvent the wheel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    \"\"\"\n",
    "    Input: str : A string to clean from html tags\n",
    "    Output: str : The same string with html tags removed\n",
    "    \"\"\"debuter le NLP\n",
    "    #TOFILL\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_dataset['review']=imdb_dataset['review'].apply(remove_html_tags)\n",
    "imdb_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    \"\"\"\n",
    "    Input: str : A string to clean from non alphanumeric characters\n",
    "    Output: str : The same strings without non alphanumeric characters\n",
    "    \"\"\"\n",
    "    #TOFILL\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_dataset['review']=imdb_dataset['review'].apply(remove_special_characters)\n",
    "imdb_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase_text(text):\n",
    "    \"\"\"\n",
    "    Input: str : A string to lowercase\n",
    "    Output: str : The same string lowercased\n",
    "    \"\"\"\n",
    "    #TOFILL\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_dataset['review']=imdb_dataset['review'].apply(lowercase_text)\n",
    "imdb_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words(text):\n",
    "    \"\"\"\n",
    "    Input: str : A string to tokenize\n",
    "    Output: list of str : A list of the tokens splitted from the input string\n",
    "    \"\"\"\n",
    "    #TOFILL\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_dataset['review']=imddebuter le NLPb_dataset['review'].apply(tokenize_words)\n",
    "imdb_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(token_list):\n",
    "    \"\"\"\n",
    "    Input: list of str : A list of tokens\n",
    "    Output: list of str : The new list with removed stopwords tokens\n",
    "    \"\"\"\n",
    "    #TOFILL\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_dataset['review']=imdb_dataset['review'].apply(remove_stopwords)\n",
    "imdb_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(token_list):\n",
    "    \"\"\"\n",
    "    Input: list of str : A list of tokens to stem\n",
    "    Output: list of str : The list of stemmed tokens\n",
    "    \"\"\"\n",
    "    #TOFILL\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debuter le NLPimdb_dataset['review']=imdb_dataset['review'].apply(stem_words)\n",
    "imdb_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's join all that together and apply it to our dataset. The following function simply chains all the preprocessing steps you just implemented. \n",
    "\n",
    "It adds the `list_output` flag, if False it will reconcatenate all the preprocessed tokens into a single string (with spaces between tokens), if True it will keep each sentence as a list of tokens. Depending on the libraries you will use for the next steps, it can be useful to have one or the other representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text_dataset(dataset, text_col_name = 'review', html_tags = True,\n",
    "                           special_chars = True, lowercase = True , stemming = True , \n",
    "                           stopwords = True, list_output = False ):\n",
    "    \"\"\"\n",
    "    Apply the choosen preprocessing steps to a corpus of texts and return the \n",
    "    preprocessed corpus. The list_output flag allows to return either a list\n",
    "    of token, or a rejoined string with spaces between the preprocessed tokens.\n",
    "    \"\"\"\n",
    "    def rejoin_text(token_list):\n",
    "        return ' '.join(token_list)\n",
    "    \n",
    "    \n",
    "    output = dataset.copy()\n",
    "    \n",
    "    if html_tags : \n",
    "        output[text_col_name] = output[text_col_name].apply(remove_html_tags)\n",
    "        \n",
    "    if special_chars :\n",
    "        output[text_col_name] = output[text_col_name].apply(remove_special_characters)\n",
    "        \n",
    "    if lowercase :\n",
    "        output[text_col_name] = output[text_col_name].apply(lowercase_text)\n",
    "    \n",
    "    #Tokenization for next steps:\n",
    "    output[text_col_name] = output[text_col_name].apply(tokenize_words)\n",
    "    \n",
    "    if stopwords :\n",
    "        output[text_col_name] = output[text_col_name].apply(remove_stopwords)\n",
    "        \n",
    "    if stemming :\n",
    "        output[text_col_name] = output[text_col_name].apply(stem_words)\n",
    "        \n",
    "    if not list_output :\n",
    "        output[text_col_name] = output[text_col_name].apply(rejoin_text)\n",
    "        \n",
    "    return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_clean_dataset = normalize_text_dataset(imdb_dataset_original, html_tags = True,\n",
    "                           special_chars = True, lowercase = True , stemming = True , \n",
    "                           stopwords = True, list_output = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Text classification with Bag-Of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Now we have cleaned the reviews of our dataset, how do we represent them as vectors in order to classify it ? \n",
    "One classic way to achieve that is the Bag-Of-Words (BOW) approach. To encode a text in a bag of word, we first need to know all the different words $w$ that appear in all our reviews, called the vocabulary : $w \\in \\mathcal{V}$. For each word $w$ we attribute an index $idx(w) = i$ with $i \\in \\{0, |\\mathcal{V}|-1\\}$, and represent a review $r$ as a vector of the size of the vocabulary $x_r \\in \\mathbb{R}^{|\\mathcal{V}|}$. To encode a review we are simply going to count how many time each word appears and assign it at its corresponding index in the bag-of-words vector : $x_{r,i} = count(w,r)$, where i = idx(w). \n",
    "\n",
    "This means that we completely disregard the words order, and simply take into account the number of times each word appears in each review to represent them. There are many variations of this concept, TF-IDF (term frequency-inverse document frequency) for example, gives more weight to uncommon words. Read more about BOW and TF-IDF there:\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2020/02/quick-introduction-bag-of-words-bow-tf-idf/\n",
    "\n",
    "Let's start with bag-of-words. In general we don't consider the whole vocabulary but only some of the most frequent words in order to reduce the dimensionality and avoid noise from rare words. Here we will only consider the 25000 most frequent words of the training set, meaning the words that are only in the test set will be ignored. Thus we have : $x_r \\in \\mathbb{R^{25000}}$.\n",
    "\n",
    "Encode all the reviews as bag-of-words, and train and evaluate a logistic regression model on the following train test splits. As we have seen previously, if we wanted to investigate this model we should also grid search for hyperparameters by doing a cross-validation with validation sets, etc. However this is not the goal today, so we'll simply go for a train/test split for this experiment. Concerning the evaluation metrics, in this case we care equally about correctly predicting the positives and the negatives, and we have a balanced dataset, thus we can simply use accuracy this time.\n",
    "\n",
    "Once again, don't do everything from scratch and try to find libraries that propose implementations of these concepts !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 25000 \n",
    "\n",
    "#Train/test split:\n",
    "\n",
    "lb=LabelBinarizer()\n",
    "sentiment_labels=lb.fit_transform(imdb_clean_dataset['sentiment'])\n",
    "\n",
    "train_reviews = imdb_clean_dataset.review[:45000]\n",
    "test_reviews = imdb_clean_dataset.review[45000:]\n",
    "\n",
    "train_sentiments = sentiment_labels[:45000]\n",
    "test_sentiments = sentiment_labels[45000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOFILL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get about 88% accuracy, pretty good for such a simple model. Now let's do the same with a tf-idf encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOFILL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "And you should get about 90% accuracy this time. Other classic but more sophisticated features include N-grams, part-of-speech tagging and syntax trees, you can read more about these there:\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2020/07/part-of-speechpos-tagging-dependency-parsing-and-constituency-parsing-in-nlp/\n",
    "\n",
    "But we will stop there for the classic approaches and go to deep learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...unless you are ahead of time, in this case learn about Bags of N-grams by yourself, and try them out :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOFILL (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks : Long Short Term Memory networks (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already covered feed-forward neural networks during the computer vision and the recommended system module. For natural language processing, one type of popular deep learning architecture is called Reccurent Neural Networks (RNNs). RNNs differ from feed-forward networks in the sense that some of their inner layers are recursively updated while iterating over the sequence of words given in input. We are going to use one specific RNN architecture called Long-Short Term Memory networks (LSTMs), which have been especially successful in various NLP tasks, including automatic translation, question answering, ... and text classification, our case study in this module.\n",
    "\n",
    "Learn more about how RNNs and LSTMs encode texts as vectors first:\n",
    "\n",
    "https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9\n",
    "\n",
    "If you want to understand in depth how one LSTM cell is working, you can go through these two articles:\n",
    "\n",
    "https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21\n",
    "\n",
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using sequential models such as LSTMs, stopwords, punctuation and words suffixes carry semantics, and have thus much more importance than when using BOW-based models. Hence with these models we will only remove html tags, and keep all these :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_deep_clean_dataset = normalize_text_dataset(imdb_dataset_original, html_tags = True,\n",
    "                           special_chars = False, lowercase = True, stemming = False, \n",
    "                           stopwords = False, list_output = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_deep_clean_dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping a validation set for early stopping is a good habit when training deep models, so let's resplit the dataset, and save the splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_deep_clean = imdb_deep_clean_dataset.iloc[:40000]\n",
    "valid_deep_clean = imdb_deep_clean_dataset.iloc[40000:45000]\n",
    "test_deep_clean = imdb_deep_clean_dataset.iloc[45000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = '../data/imdb_clean/'\n",
    "if not os.path.exists(outdir):\n",
    "    os.mkdir(outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_deep_clean.to_csv(outdir + 'train.csv', index = False)\n",
    "valid_deep_clean.to_csv(outdir + 'valid.csv', index = False)\n",
    "test_deep_clean.to_csv(outdir + 'test.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can restart the code from there in case of a crash :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = '../data/imdb_clean/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "train_deep_clean = pd.read_csv(outdir + 'train.csv')\n",
    "valid_deep_clean = pd.read_csv(outdir + 'valid.csv')\n",
    "test_deep_clean = pd.read_csv(outdir + 'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing LSTMs with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some lines that allow for faster training with this version of tensorflow for these models\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Keras to implement an LSTM network. First we need to encode all the reviews as a list of indexes, where each word is replaced by its embedding index using keras \"Tokenizer\". To make all training reviews the same size, we will make them the same size as the longest review and add the special token < pad > as many time as necessary to all the other ones with the function `pad_sequences`. This token will be ignored by the LSTM layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 25000 \n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_vocab_size, split=' ', oov_token='<unw>', filters=' ')\n",
    "tokenizer.fit_on_texts(pd.concat([train_deep_clean,valid_deep_clean]).review)\n",
    "\n",
    "# This encodes our sentence as a sequence of integer\n",
    "# each integer being the index of each word in the vocabulary\n",
    "train_seqs = tokenizer.texts_to_sequences(train_deep_clean.review)\n",
    "valid_seqs = tokenizer.texts_to_sequences(valid_deep_clean.review)\n",
    "test_seqs = tokenizer.texts_to_sequences(test_deep_clean.review)\n",
    "\n",
    "# We need to pad the sequences so that they are all the same length :\n",
    "# the length of the longest one\n",
    "max_seq_length = max( [len(seq) for seq in train_seqs + valid_seqs] )\n",
    "\n",
    "X_train = pad_sequences(train_seqs, max_seq_length)\n",
    "X_valid = pad_sequences(valid_seqs, max_seq_length)\n",
    "X_test = pad_sequences(test_seqs, max_seq_length)\n",
    "\n",
    "y_train = pd.get_dummies(train_deep_clean.sentiment).values[:,1]\n",
    "y_valid = pd.get_dummies(valid_deep_clean.sentiment).values[:,1]\n",
    "y_test = pd.get_dummies(test_deep_clean.sentiment).values[:,1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fill the following function to implement a simple LSTM model : one embedding layer, one LSTM layer, and a final dense layer that yields a single score with a sigmoid activation function. Use Keras' Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lstm_model(vocab_size, embedding_dim, seq_length, lstm_out_dim):\n",
    "    #TOFILL\n",
    "    \n",
    "    \n",
    "    #TOKEEP\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer='SGD',metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "lstm_out_dim = 200  #Bigger than embedding dim, as it combines all the words of each review\n",
    "\n",
    "model = get_lstm_model(max_vocab_size, embedding_dim, max_seq_length, lstm_out_dim)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "max_epochs = 2\n",
    "history = model.fit(X_train, y_train, epochs=max_epochs, batch_size=batch_size, \n",
    "                    verbose=1, validation_data = (X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = model.evaluate(X_test, y_test, verbose=0) \n",
    "print(\"Test accuracy: %.2f%%\" % (test_acc[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty low accuracy isn't it ? Actually it is very easy to incorrectly train a deep neural net. Change the optimizer with \"adam\" instead of \"SGD\", add a dropout layer after the LSTM layer for regularization, and use early stopping :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropout, early stopping, adam\n",
    "def get_lstm_model_2(vocab_size, embedding_dim, seq_length, lstm_out_dim, dropout_rate):\n",
    "    #TOFILL\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "lstm_out_dim = 200\n",
    "dropout_rate = 0.2\n",
    "\n",
    "model = get_lstm_model_2(max_vocab_size, embedding_dim, max_seq_length, lstm_out_dim, dropout_rate)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOFILL\n",
    "early_stopping = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "max_epochs = 5\n",
    "history = model.fit(X_train, y_train, epochs=max_epochs, batch_size=batch_size, \n",
    "                    verbose=1, validation_data = (X_valid, y_valid), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = model.evaluate(X_test, y_test, verbose=0) \n",
    "print(\"Test accuracy: %.2f%%\" % (test_acc[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better. If we'd run for a longer time, we'd get a bit better results from our classic methods, but that's still quite slow for little improvement. We could also grid search for all hyper parameters (embedding and layer sizes, dropout rate, ...), but that's not the goal today, remember however that grid-search is standard when optimizing a model predictive performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict sentiment for arbitrary sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can try predict the sentiment of any kind of sentence in english, try your own. You first need to encode each review as a sequence of indexes (called tokens in keras), to pad these sequances, and finally predict the score with your trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good = \"i really liked the movie and had fun\"\n",
    "bad = \"worst movie on the planet , so boring\"\n",
    "for review in [good,bad]:\n",
    "    \n",
    "    #TOFILL\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize embeddings with pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training of LSTMs is a bit heavy, one way to speed this up is to re-use pre-trained word embeddings. Many such embeddings are available on the net. Read this to understand how are produced word embeddings and why they encode information that helps with all NLP tasks:\n",
    "\n",
    "http://jalammar.github.io/illustrated-word2vec/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use GloVe embeddings, download and load the embeddings produced from 6 billions documents from : https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open('../data/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('%s unique words in vocabulary' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our word index, search for each of our 25000 most frequents words if they exist in the pretrained GloVe embeddings and assign them to their corresponding row index in the embedding matrix. If they don't exist in the GloVe embeddings, assign a random vector :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "# Allocate the embeddings matrix\n",
    "embedding_matrix = np.zeros((max_vocab_size, embedding_dim))\n",
    "\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    #TOFILL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now change your LSTM model so that the embedding layer is initialized with the pretrained embeddings :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lstm_model_pretrained_embs(vocab_size, embedding_dim, seq_length, \n",
    "                                   lstm_out_dim, dropout_rate, embedding_matrix):\n",
    "    #TOFILL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "lstm_out_dim = 200\n",
    "dropout_rate = 0.2\n",
    "\n",
    "model = get_lstm_model_pretrained_embs(max_vocab_size, embedding_dim, max_seq_length, \n",
    "                                       lstm_out_dim, dropout_rate, embedding_matrix)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "max_epochs = 5\n",
    "history = model.fit(X_train, y_train, epochs=max_epochs, batch_size=batch_size, \n",
    "                    verbose=1, validation_data = (X_valid, y_valid), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = model.evaluate(X_test, y_test, verbose=0) \n",
    "print(\"Test accuracy: %.2f%%\" % (test_acc[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the validation accuracy indeed progressed much faster than previously.\n",
    "\n",
    "For more speed-up, at the price of accuracy, let's fix the embeddings so that they are not trainable parameters of the model, meaning they won't be updated during training :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lstm_model_pretrained_embs(vocab_size, embedding_dim, seq_length, \n",
    "                                   lstm_out_dim, dropout_rate, embedding_matrix,\n",
    "                                    trainable_embeddings):\n",
    "    #TOFILL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "lstm_out_dim = 200\n",
    "dropout_rate = 0.2\n",
    "trainable_embeddings = False\n",
    "\n",
    "model = get_lstm_model_pretrained_embs(max_vocab_size, embedding_dim, max_seq_length, \n",
    "                                       lstm_out_dim, dropout_rate, embedding_matrix, trainable_embeddings)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the change in the number of trainable parameters in the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "max_epochs = 5\n",
    "history = model.fit(X_train, y_train, epochs=max_epochs, batch_size=batch_size, \n",
    "                    verbose=1, validation_data = (X_valid, y_valid), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = model.evaluate(X_test, y_test, verbose=0) \n",
    "print(\"Test accuracy: %.2f%%\" % (test_acc[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By fixing the word embeddings, the training time shrunk a bit, but the validation accuracy is progressing more slowly and reaching a limit. Depending on the network architecture, the trade-off can be interesting, here not so much, just know this is a possibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â Going Further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional and stacked LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTMs parse the text from left to right, but doing it also from right to left and concatening the two output vectors improved the results. These are called bidirectional LSTMs. It is also possible to stack multiple LSTM layers.\n",
    "\n",
    "This image is a good illustration of how these two variants work:\n",
    "\n",
    "https://www.researchgate.net/figure/Illustrations-for-basic-LSTMs-and-the-three-layer-stacked-LSTM-model-for-the-sequential_fig3_313115860\n",
    "\n",
    "\n",
    "First modify your network to make a bidirectional LSTM :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bilstm_model_pretrained_embs(vocab_size, embedding_dim, seq_length, \n",
    "                                     lstm_out_dim, dropout_rate, embedding_matrix,\n",
    "                                    trainable_embeddings):\n",
    "    #TOFILL\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "lstm_out_dim = 200\n",
    "dropout_rate = 0.2\n",
    "\n",
    "model = get_bilstm_model_pretrained_embs(max_vocab_size, embedding_dim, max_seq_length, \n",
    "                                       lstm_out_dim, dropout_rate, embedding_matrix, True)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "max_epochs = 5\n",
    "history = model.fit(X_train, y_train, epochs=max_epochs, batch_size=batch_size, \n",
    "                    verbose=1, validation_data = (X_valid, y_valid), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = model.evaluate(X_test, y_test, verbose=0) \n",
    "print(\"Test accuracy: %.2f%%\" % (test_acc[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try stacking multiple bidirectional LSTM layers, where the number of layers `n_layers` is a parameter of the function building the model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multilayer_bilstm_model_pretrained_embs(vocab_size, embedding_dim, seq_length, \n",
    "                                                lstm_out_dim, dropout_rate, embedding_matrix,\n",
    "                                                trainable_embeddings, n_layers):\n",
    "    #TOFILL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "lstm_out_dim = 100\n",
    "dropout_rate = 0.2\n",
    "n_layers = 2\n",
    "\n",
    "model = get_multilayer_bilstm_model_pretrained_embs(max_vocab_size, embedding_dim, max_seq_length, \n",
    "                                       lstm_out_dim, dropout_rate, embedding_matrix, True, n_layers)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "max_epochs = 5\n",
    "history = model.fit(X_train, y_train, epochs=max_epochs, batch_size=batch_size, \n",
    "                    verbose=1, validation_data = (X_valid, y_valid), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = model.evaluate(X_test, y_test, verbose=0) \n",
    "print(\"Test accuracy: %.2f%%\" % (test_acc[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the max accuracy reached is not much better than our TF-IDF model. This happens because the full word order is actually not so important for Sentiment Analysis. For this task, Convolutional Neural Networks can attain comparable performances faster, as they have simpler architectures. But that's not true for other task such as translation, question answering, ... (which are tasks that are a bit too long to train to be included in this course, hence the choice of sentiment an analysis to practice RNNs).\n",
    "\n",
    "Let's do it with a convolutional model by using 1D convolution with a kernel size of 3 over the word embeddings (this means that it will convolve the embeddings of the consecutive words 3 by 3), followed by a 1D max pooling and a dense ReLU layer before the final sigmoid :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv_model_pretrained_embs(vocab_size, embedding_dim, seq_length, \n",
    "                                                out_dim, dropout_rate, embedding_matrix,\n",
    "                                                trainable_embeddings):\n",
    "    #TOFILL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "out_dim = 200\n",
    "dropout_rate = 0.2\n",
    "\n",
    "model = get_conv_model_pretrained_embs(max_vocab_size, embedding_dim, max_seq_length, \n",
    "                                       out_dim, dropout_rate, embedding_matrix, True)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "max_epochs = 5\n",
    "history = model.fit(X_train, y_train, epochs=max_epochs, batch_size=batch_size, \n",
    "                    verbose=1, validation_data = (X_valid, y_valid), callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = model.evaluate(X_test, y_test, verbose=0) \n",
    "print(\"Test accuracy: %.2f%%\" % (test_acc[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going even further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following parts are meant to be resources to explore if you are interested in the advanced concept of attention in deep nets. There are explanation links, as well as links with code for each of them, but don't feel obliged to implement all of them, these are meant to help understanding each of these concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention is a mechanism that changes the output of an LSTM : instead of outputting the final hidden state vector $h_n$ where $n$ is the length of the encoded text, attention plugs on top of a LSTM and returns a combination of all the hidden state vectors at each word position $\\ \\sum_{t=1}^n \\alpha_t h_t$ (where $\\alpha_t \\in (0,1))$, and thus allows to pay a different attention to each part of the text, hence the name. \n",
    "\n",
    "It has been originally proposed for sequence to sequence models, like translation models, where there is a different attention combination computed for each translated output word. It is thus less useful for text classification, but it can be adapted, by computing a single output combination of all the hidden states, as explained in Section 3.3 of the following article :\n",
    "\n",
    "https://www.aclweb.org/anthology/P16-2034.pdf\n",
    "\n",
    "Here is a link about how to apply attention for text classification with Keras:\n",
    "\n",
    "https://www.kaggle.com/yshubham/simple-lstm-for-text-classification-with-attention\n",
    "\n",
    "\n",
    "You can also read the following link to understand how attention works in sequence to sequence models, which are nothing more than a reversed LSTM (the decoder) on top of a first LSTM (the encoder), in this case for translation where it helps aligning words in two different languages :\n",
    "\n",
    "https://towardsdatascience.com/day-1-2-attention-seq2seq-models-65df3f49e263"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer architecture for text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State of the art models in NLP are not RNNs anymore, but Transformers. Transformers do not read text sequentially like RNNs, the core concept of Transformers is self-attention, an attention mechanism that combine separately each word embedding with the other word embeddings of the text. There are multiple such attention mechanisms called \"attention heads\" in a layer, and multiple such layers are stacked.\n",
    "\n",
    "Read this article to understand the self-attention layer:\n",
    "\n",
    "https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a\n",
    "\n",
    "This article explains very well the Transformer for sequence to sequence models (again remember that a text classification model is just the encoder part of a sequence to sequence model) :\n",
    "\n",
    "http://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "Keras code to do text classification with a Transformer :\n",
    "\n",
    "https://keras.io/examples/nlp/text_classification_with_transformer/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current state-of-the-art performance for text classification are achieved by doing transfer learning from the BERT model. The BERT model combines different techniques including the Transformer to pretrain in an unsupervised fashion on plain text. The last layers of BERT provide a high-level contextual representation of english sentences, and can then be reused in any NLP deep model. The website HuggingFace ( https://huggingface.co/ ) hosts many pretrained deep learning models that can be reused and fine-tuned for your application.\n",
    "\n",
    "The BERT model : http://jalammar.github.io/illustrated-bert/\n",
    "\n",
    "Reusing a pretrained BERT model from HuggingFace with Keras for text classification : https://www.kaggle.com/code/satyampd/bert-text-classification-w-keras-huggingface/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT in pytorch with skorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to familizarize yourself with pytorch, one easy way when one is used to scikit-learn and keras is to use the `skorch` library. `skorch` implements a high level interface to pytorch with the usual `fit` and `predict` functions : https://skorch.readthedocs.io/en/stable/user/quickstart.html\n",
    "\n",
    "You can first try to reimplement a simple LSTM with pytorch. Then you can try to redo the prediction with a BERT model from huggingface : https://nbviewer.org/github/skorch-dev/skorch/blob/master/notebooks/Hugging_Face_Finetuning.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying OpenAI (ChatGPT, GPT, ...) models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The largest language models available nowadays like the latest versions of GPT and ChatGPT do not fit on a typical computer, but we can query them through OpenAI API to use them. However this is not free :\n",
    "https://openai.com/pricing\n",
    "\n",
    "If you feel like you want to spend some money for this, we can try two different ways. In both cases we can use the `langchain` package to interact with the openAI API through python code. \n",
    "\n",
    "The first option is to get embeddings for each text from the Ada2 model, and then build a classifier from the embeddings as features  :\n",
    "https://shishirsingh66g.medium.com/langchain-applications-part-3-embedding-models-75e6a0d01545\n",
    "\n",
    "Another one is simply to ask ChatGPT whether it thinks each review of the test set is positive or negative, and compute its accuracy. Look at `langchain` quickstart guide to do so : https://python.langchain.com/docs/get_started/quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
